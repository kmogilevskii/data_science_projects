## Проблема задачи:

Количество слов в предложении на исходном языке чаще всего будет отличным от количества слов в переводе. В RNN на одно входное слово мы всегда получаем новое скрытое состояние (hidden state), что всегда приводит к одному выходному слову. Следовательно, если мы будем сохранять все скрытые состояния, то получим, что выходная последовательность будет иметь такую же длину, как и входная последовательность.
## Решение:

Архитектура seq2seq: двойная RNN система. Первая RNN называется Encoder, а вторая – Decoder. Берется сырой input в виде текста, аудио или изображения и мы создаём некоторое векторное представление из него (работа Encoder’а). Затем генерируются новые данные из полученного сжатого векторного представления (Decoder). 
В простейшей реализации данной архитектуры, Encoder возвращает последнее скрытое состояние (thought vector) и передает его Decoder’у. Однако, в данном проекте для улучшения работы модели мы добавляем механизм attention. 
Данный механизм позволяет нам использовать все скрытые состояния, а не самый последний и к тому же взвешивать их по важности на каждом шаге генерации выходной последовательности. Вычисляются веса каждого состояния с помощью небольшой нейронной сети, которая на вход получает каждое состояние сконкатенированное с предыдущим скрытым состоянием Decoder’а. В результате на вход в Decoder идет специальный вектор, называемый контекстом. В случае с обучением Decoder’а через teacher forcing нам надо добавить к контексту правильное слово, которое модель должна была выдать на предыдущем шаге. 
