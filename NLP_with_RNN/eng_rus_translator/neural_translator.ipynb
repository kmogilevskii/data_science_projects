{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронный переводчик с английского на русский\n",
    "\n",
    "В данном проекте мы будем реализовывать архитектуру seq2seq с механизмом attention. Более подробное описание всей работы архитектуры можно найти в текстовом файле. Здесь я буду писать кратко.\n",
    "\n",
    "Сперва импортируем все необходимые библиотеки, напишем свою версию функции softmax и определим некоторые константы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "import os, sys\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, \\\n",
    "  Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import keras.backend as K\n",
    "    if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
    "        from keras.layers import CuDNNLSTM as LSTM\n",
    "        from keras.layers import CuDNNGRU as GRU\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# softmax должна вычисляться по временной оси\n",
    "# т.к. дефолтная реализация ожидает, что временная ось находится в конце\n",
    "# то нам нужно реализовать данную функцию самостоятельно\n",
    "# ожидаемые размерности N x T x D (время по середине)\n",
    "# note: the latest version of Keras allows you to pass in axis arg\n",
    "def softmax_over_time(x):\n",
    "    assert(K.ndim(x) > 2)\n",
    "    e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
    "    s = K.sum(e, axis=1, keepdims=True)\n",
    "    return e / s\n",
    "\n",
    "\n",
    "\n",
    "# константы\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LATENT_DIM = 400\n",
    "LATENT_DIM_DECODER = 500 # надо проверить, что будет работать с разным количеством нейронов\n",
    "NUM_SAMPLES = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет, который хранит в себе большое количество фраз на английском и их соответствующий перевод на русский. На его основе создадим 3 списка: в первом хранятся фразы на английском языке, во втором соответствующий перевод на русском плюс специальный токен сигнализирующий окончание предложения, а в третьем тоже соответствующие переводы только с добавлением токена о начале предложения. Последний нужен для того, чтобы обучить decoder через teacher forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество фраз: 20000\n"
     ]
    }
   ],
   "source": [
    "input_texts = [] \n",
    "target_texts = [] \n",
    "target_texts_inputs = [] \n",
    "\n",
    "\n",
    "t = 0\n",
    "for line in open('rus.txt',encoding='utf-8'):\n",
    "  # возьмем данные не из всего датасета\n",
    "    t += 1\n",
    "    if t > NUM_SAMPLES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    input_text, translation, *rest = line.rstrip().split('\\t')\n",
    "\n",
    "    target_text = translation + ' <eos>'\n",
    "    target_text_input = '<sos> ' + translation\n",
    "\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    target_texts_inputs.append(target_text_input)\n",
    "print(\"Количество фраз:\", len(input_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующим шагом будет создание токенизатора для каждого языка отдельно, определение словаря с парами слово-индекс и определение максимальной длины фразы как входного предложения, так и перевода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 2803 уникальных слов в Английском\n"
     ]
    }
   ],
   "source": [
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
    "\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print(f'Найдено {len(word2idx_inputs)} уникальных слов в Английском')\n",
    "\n",
    "max_len_input = max(len(s) for s in input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 9896 уникальных слов в Русском\n"
     ]
    }
   ],
   "source": [
    "# filters='' для того, чтобы токенизатор учел специальные токены\n",
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) \n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n",
    "\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "print(f'Найдено {len(word2idx_outputs)} уникальных слов в Русском')\n",
    "\n",
    "# сохраним число уникальных слов + 1 на будущее\n",
    "# модель к каждому такому уникальному слову будет приписывать вероятность на основе которой она будет делать предсказания\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "\n",
    "max_len_target = max(len(s) for s in target_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно сделать паддинг всех последовательностей до найденной максимальной длины для обоих языков.\n",
    "\n",
    "Для Encoder'а добавляем нули вначале, чтобы ему не нужно было много запоминать. Для Decoder'а логичнее сделать паддинг в конце, так как перевод не может начаться с пустых символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_data.shape: (20000, 5)\n",
      "encoder_data[0]: [ 0  0  0  0 11]\n",
      "decoder_data[0]: [   2 4616    0    0    0    0    0    0    0    0    0]\n",
      "decoder_data.shape: (20000, 11)\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print(\"encoder_data.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_data[0]:\", encoder_inputs[0])\n",
    "\n",
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
    "print(\"decoder_data[0]:\", decoder_inputs[0])\n",
    "print(\"decoder_data.shape:\", decoder_inputs.shape)\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно загрузить предобученные embeddings для английского языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 400000 векторных представлений слов.\n"
     ]
    }
   ],
   "source": [
    "word2vec = {}\n",
    "for line in open(f'glove.6B.{EMBEDDING_DIM}d.txt', encoding='utf-8'):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "print(f'Найдено {len(word2vec)} векторных представлений слов.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из полученных эмбеддингов создадим специальную матрицу, где индекс строки будет соответствовать индексу в словаре word2idx_inputs, что позволит эффективно извлекать соответствующий слову вектор на основе его уникального индекса, данного токенизатором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if i < MAX_NUM_WORDS:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # ненайденные слова в матрице буду иметь нулевой вектор\n",
    "      embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим Embedding layer для Encoder'а и преобразуем целевой признак в OHE, так как не получается использовать sparse_categorical_crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=max_len_input,\n",
    ")\n",
    "\n",
    "decoder_targets_one_hot = np.zeros(\n",
    "  (\n",
    "    len(input_texts),\n",
    "    max_len_target,\n",
    "    num_words_output\n",
    "  ),\n",
    "  dtype='float32'\n",
    ")\n",
    "\n",
    "for i, d in enumerate(decoder_targets):\n",
    "    for t, word in enumerate(d):\n",
    "        if word > 0:\n",
    "            decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создаем модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# сначала Encoder\n",
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = Bidirectional(LSTM(LATENT_DIM,return_sequences=True))\n",
    "encoder_outputs = encoder(x)\n",
    "\n",
    "# теперь Decoder\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
    "decoder_embedding = Embedding(num_words_output,EMBEDDING_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_repeat_layer = RepeatVector(max_len_input)\n",
    "attn_concat_layer = Concatenate(axis=-1)\n",
    "attn_dense1 = Dense(10,activation='tanh')\n",
    "attn_dense2 = Dense(1,activation=softmax_over_time)\n",
    "attn_dot = Dot(axes=1)\n",
    "\n",
    "def one_step_attention(h, st_1):\n",
    "    # h = h(1), ..., h(Tx), shape = (Tx, LATENT_DIM * 2)\n",
    "    # st_1 = s(t-1), shape = (LATENT_DIM_DECODER,)\n",
    " \n",
    "    # копируем s(t-1) Tx раз\n",
    "    # теперь shape = (Tx, LATENT_DIM_DECODER)\n",
    "    st_1 = attn_repeat_layer(st_1)\n",
    "\n",
    "    # Concatenate all h(t)'s with s(t-1)\n",
    "    # Теперь shape (Tx, LATENT_DIM_DECODER + LATENT_DIM * 2)\n",
    "    x = attn_concat_layer([h, st_1])\n",
    "\n",
    "    x = attn_dense1(x)\n",
    "\n",
    "    alphas = attn_dense2(x)\n",
    "\n",
    "    # непосредственно вычисляем контекст\n",
    "    context = attn_dot([alphas, h])\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь надо написать оставшийся Decoder после реализации attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_lstm = LSTM(LATENT_DIM_DECODER, return_state=True)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "\n",
    "initial_s = Input(shape=(LATENT_DIM_DECODER,), name='s0')\n",
    "initial_c = Input(shape=(LATENT_DIM_DECODER,), name='c0')\n",
    "# для teacher forcing: комбинируем предыдущее правильное слово с текущим контекстом\n",
    "context_last_word_concat_layer = Concatenate(axis=2)\n",
    "\n",
    "s = initial_s\n",
    "c = initial_c\n",
    "\n",
    "outputs = []\n",
    "for t in range(max_len_target): # Ty раз\n",
    "    # вычисляем контекст с использованием attention\n",
    "    context = one_step_attention(encoder_outputs, s)\n",
    "\n",
    "    # мы не хотим конкатенировать контекст со всей входной последовательностью для teacher forcing\n",
    "    # для 1 шага в генерировании выходного слова нам нужно взять лишь 1 слово (правильное на предыдущем шаге)\n",
    "    selector = Lambda(lambda x: x[:, t:t+1])\n",
    "    xt = selector(decoder_inputs_x)\n",
    "  \n",
    "    # комбинируем \n",
    "    decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
    "\n",
    "    # передаем комбинированные [контекст, последнее слово] в LSTM\n",
    "    # вместе с [s, c]\n",
    "    # получаем новые [s, c] и output\n",
    "    o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
    "\n",
    "    decoder_outputs = decoder_dense(o)\n",
    "    outputs.append(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'outputs' это список длиной Ty\n",
    "# каждый элемент имеет размер (размер батча, словарь языка на который переводим (русский))\n",
    "# нам надо преобразовать этот список в 1 тензор\n",
    "# если просто использовать stack, то получим T x N x D\n",
    "# а нам нужно вот так N x T x D\n",
    "\n",
    "def stack_and_transpose(x):\n",
    "    # x это список длиной Ty, каждый элемент batch_size x output_vocab_size тензор\n",
    "    x = K.stack(x) # теперь Ty x batch_size x output_vocab_size tensor\n",
    "    x = K.permute_dimensions(x, pattern=(1, 0, 2)) # теперь batch_size x T x output_vocab_size\n",
    "    return x\n",
    "\n",
    "# сделаем из данной функции слой, так как так хочет Keras\n",
    "stacker = Lambda(stack_and_transpose)\n",
    "outputs = stacker(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/30\n",
      "16000/16000 [==============================] - 247s 15ms/step - loss: 5.3982 - acc: 0.3393 - val_loss: 5.1719 - val_acc: 0.2978\n",
      "Epoch 2/30\n",
      "16000/16000 [==============================] - 233s 15ms/step - loss: 4.2391 - acc: 0.3964 - val_loss: 4.7837 - val_acc: 0.3925\n",
      "Epoch 3/30\n",
      "16000/16000 [==============================] - 232s 15ms/step - loss: 3.6410 - acc: 0.4594 - val_loss: 4.5203 - val_acc: 0.4380\n",
      "Epoch 4/30\n",
      "16000/16000 [==============================] - 246s 15ms/step - loss: 3.0892 - acc: 0.5048 - val_loss: 4.3843 - val_acc: 0.4687\n",
      "Epoch 5/30\n",
      "16000/16000 [==============================] - 218s 14ms/step - loss: 2.5971 - acc: 0.5433 - val_loss: 4.2744 - val_acc: 0.4876\n",
      "Epoch 6/30\n",
      "16000/16000 [==============================] - 218s 14ms/step - loss: 2.1474 - acc: 0.5851 - val_loss: 4.2102 - val_acc: 0.5039\n",
      "Epoch 7/30\n",
      "16000/16000 [==============================] - 217s 14ms/step - loss: 1.7441 - acc: 0.6278 - val_loss: 4.1840 - val_acc: 0.5134\n",
      "Epoch 8/30\n",
      "16000/16000 [==============================] - 217s 14ms/step - loss: 1.4089 - acc: 0.6671 - val_loss: 4.1340 - val_acc: 0.5209\n",
      "Epoch 9/30\n",
      "16000/16000 [==============================] - 238s 15ms/step - loss: 1.1439 - acc: 0.7050 - val_loss: 4.1089 - val_acc: 0.5271\n",
      "Epoch 10/30\n",
      "16000/16000 [==============================] - 220s 14ms/step - loss: 0.9424 - acc: 0.7377 - val_loss: 4.1167 - val_acc: 0.5330\n",
      "Epoch 11/30\n",
      "16000/16000 [==============================] - 226s 14ms/step - loss: 0.7997 - acc: 0.7593 - val_loss: 4.1067 - val_acc: 0.5395\n",
      "Epoch 12/30\n",
      "16000/16000 [==============================] - 215s 13ms/step - loss: 0.6999 - acc: 0.7752 - val_loss: 4.1277 - val_acc: 0.5354\n",
      "Epoch 13/30\n",
      "16000/16000 [==============================] - 219s 14ms/step - loss: 0.6238 - acc: 0.7868 - val_loss: 4.1410 - val_acc: 0.5393\n",
      "Epoch 14/30\n",
      "16000/16000 [==============================] - 226s 14ms/step - loss: 0.5730 - acc: 0.7953 - val_loss: 4.1360 - val_acc: 0.5413\n",
      "Epoch 15/30\n",
      "16000/16000 [==============================] - 251s 16ms/step - loss: 0.5368 - acc: 0.7985 - val_loss: 4.1463 - val_acc: 0.5435\n",
      "Epoch 16/30\n",
      "16000/16000 [==============================] - 237s 15ms/step - loss: 0.5057 - acc: 0.8039 - val_loss: 4.1675 - val_acc: 0.5463\n",
      "Epoch 17/30\n",
      "16000/16000 [==============================] - 242s 15ms/step - loss: 0.4828 - acc: 0.8059 - val_loss: 4.1757 - val_acc: 0.5460\n",
      "Epoch 18/30\n",
      "16000/16000 [==============================] - 248s 15ms/step - loss: 0.4669 - acc: 0.8085 - val_loss: 4.1947 - val_acc: 0.5441\n",
      "Epoch 19/30\n",
      "16000/16000 [==============================] - 240s 15ms/step - loss: nan - acc: 0.2879 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/30\n",
      "16000/16000 [==============================] - 229s 14ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/30\n",
      "16000/16000 [==============================] - 214s 13ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "16000/16000 [==============================] - 233s 15ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/30\n",
      "16000/16000 [==============================] - 238s 15ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/30\n",
      "16000/16000 [==============================] - 241s 15ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/30\n",
      "16000/16000 [==============================] - 235s 15ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/30\n",
      "16000/16000 [==============================] - 214s 13ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "16000/16000 [==============================] - 228s 14ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/30\n",
      "16000/16000 [==============================] - 239s 15ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "16000/16000 [==============================] - 229s 14ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/30\n",
      "16000/16000 [==============================] - 230s 14ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "  inputs=[\n",
    "    encoder_inputs_placeholder,\n",
    "    decoder_inputs_placeholder,\n",
    "    initial_s, \n",
    "    initial_c,\n",
    "  ],\n",
    "  outputs=outputs\n",
    ")\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # both are of shape N x T x K\n",
    "    mask = K.cast(y_true > 0, dtype='float32')\n",
    "    out = mask * y_true * K.log(y_pred)\n",
    "    return -K.sum(out) / K.sum(mask)\n",
    "\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "    # both are of shape N x T x K\n",
    "    targ = K.argmax(y_true, axis=-1)\n",
    "    pred = K.argmax(y_pred, axis=-1)\n",
    "    correct = K.cast(K.equal(targ, pred), dtype='float32')\n",
    "\n",
    "    # 0 is padding, don't include those\n",
    "    mask = K.cast(K.greater(targ, 0), dtype='float32')\n",
    "    n_correct = K.sum(mask * correct)\n",
    "    n_total = K.sum(mask)\n",
    "    return n_correct / n_total\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=[acc])\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# train the model\n",
    "z = np.zeros((len(encoder_inputs), LATENT_DIM_DECODER)) # initial [s, c]\n",
    "r = model.fit(\n",
    "  [encoder_inputs, decoder_inputs, z, z], decoder_targets_one_hot,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно отдельно создать модель для предсказаний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n",
    "\n",
    "# посколько encoder является bidirectional, то на каждом шаге будет два hidden state\n",
    "encoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "# нет нужды в цикле, так как тут только 1 шаг\n",
    "context = one_step_attention(encoder_outputs_as_input, initial_s)\n",
    "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n",
    "\n",
    "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
    "decoder_outputs = decoder_dense(o)\n",
    "\n",
    "decoder_model = Model(\n",
    "  inputs=[\n",
    "    decoder_inputs_single,\n",
    "    encoder_outputs_as_input,\n",
    "    initial_s, \n",
    "    initial_c\n",
    "  ],\n",
    "  outputs=[decoder_outputs, s, c]\n",
    ")\n",
    "\n",
    "# так как модель будет выдавать не слова, а их индексы, то нужно будет их преобразовывать в слова\n",
    "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # кодируем входную последовательность\n",
    "    enc_out = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1))\n",
    "  \n",
    "    # начинается перевод со специального токена\n",
    "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "\n",
    "    # выходим из цикла, когда встречаем этот токен\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "\n",
    "\n",
    "    # [s, c] будут обновляться на каждой итерации\n",
    "    s = np.zeros((1, LATENT_DIM_DECODER))\n",
    "    c = np.zeros((1, LATENT_DIM_DECODER))\n",
    "\n",
    "\n",
    "    # Перевод\n",
    "    output_sentence = []\n",
    "    for _ in range(max_len_target):\n",
    "        o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
    "        \n",
    "\n",
    "        # получаем следующее слово\n",
    "        idx = np.argmax(o.flatten())\n",
    "\n",
    "        # проверяем конец ли последовательности\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            word = idx2word_trans[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        # обновляем входное слово в декодер на следующей итерации\n",
    "        target_seq[0, 0] = idx\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # проверка\n",
    "    i = np.random.choice(len(input_texts))\n",
    "    input_seq = encoder_inputs[i:i+1]\n",
    "    translation = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Входное предложение:', input_texts[i])\n",
    "    print('Предсказанный перевод:', translation)\n",
    "    print('Истинный перевод:', target_texts[i])\n",
    "\n",
    "    ans = input(\"Продолжить? [Y/n]\")\n",
    "    if ans and ans.lower().startswith('n'):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
