{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Translator from English to Russian\n",
    "\n",
    "In case of using RNN we always get one hidden state per input and that always translates to one output per input. If we keep all the outputs at each point in time, we would get output length equal to input length. Here we can clearly see the problem: most certantly the translation will contain different number of words, than the input sentence. \n",
    "\n",
    "Solution would be using dual RNN or **seq2seq** architecture. First RNN, that takes the input is called **Encoder**, and the second, that produces translation, is called **Decoder**. Encoder takes in some raw input, like text, audio or image, and create some vector representation out of it. Then, the Decoder produces new data from this compressed vector representation. Such type of architecture can be used for any kind of request/response type of task.\n",
    "\n",
    "Encoder returns just final hidden state, thus from entire input sequence we end up with static size M vector, where M is the number of RNN units in the RNN layer. This final hidden state is called a **thought vector**. So, Encoder's job is to fold up the input sequence into small, but informative vector and Decoder's job is to unfold this vector into a new sequence.\n",
    "\n",
    "For training Decoder we would need to user **teacher forcing**, where we pass true target sequence with offset. During predictions we would use the output of previous LSTM as the input to the next one, but for training we need to feed in the true word, even when predictions of previous LSTM is incorrect. This is, basically, how human learns a language: it's better to correct the wrong use of particular word, or all future words, most probably, would be incorrect.\n",
    "\n",
    "## 1. Loading data\n",
    "\n",
    "We would split our initial dataset with a lot of translated words and phrases into input sequence for Encoder, input sequence for Decoder and target sequence for Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input sentences: 10000\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "BATCH_SIZE = 64\n",
    "LATENT_DIM = 256\n",
    "EPOCHS = 40\n",
    "MAX_NUM_WORDS = 20000\n",
    "NUM_SAMPLES = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "target_texts_input = []\n",
    "\n",
    "t = 0\n",
    "for line in open('rus.txt', encoding='utf-8'):\n",
    "    # limiting dataset\n",
    "    t += 1\n",
    "    if t > NUM_SAMPLES:\n",
    "        break\n",
    "\n",
    "    input_text, translation, *rest = line.rstrip().split('\\t')\n",
    "    target_text = translation + ' <eos>'\n",
    "    target_text_input = '<sos> ' + translation\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    target_texts_input.append(target_text_input)\n",
    "print(f'Number of input sentences: {len(input_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. Go ahead.\n",
      "<sos> Ладно, давай.\n",
      "Ладно, давай. <eos>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "random_idx = np.random.choice(len(input_texts))\n",
    "print(input_texts[random_idx])\n",
    "print(target_texts_input[random_idx])\n",
    "print(target_texts[random_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to convert or sentences in sequences of numbers with the use of Keras' Tokenizer. We would need to create two separate tokenizer, as we're dealing with 2 different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1731 unique words in english language.\n",
      "[81, 9, 417]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "\n",
    "word2idx_input = tokenizer.word_index\n",
    "print(f'Found {len(word2idx_input)} unique words in english language.')\n",
    "\n",
    "print(input_sequences[random_idx])\n",
    "\n",
    "max_len_input = max(len(s) for s in input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5923 unique words in Russian language.\n",
      "[217, 2365, 1]\n",
      "[2, 217, 2365]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS,filters='')\n",
    "tokenizer.fit_on_texts(target_texts+target_texts_input) # inefficient way to find <eos>\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "target_sequences_input = tokenizer.texts_to_sequences(target_texts_input)\n",
    "\n",
    "word2idx_output = tokenizer.word_index\n",
    "print(f'Found {len(word2idx_output)} unique words in Russian language.')\n",
    "\n",
    "print(target_sequences[random_idx])\n",
    "print(target_sequences_input[random_idx])\n",
    "\n",
    "num_words_output = len(word2idx_output) + 1\n",
    "max_len_target = max(len(s) for s in target_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've find the max length of the sequence, because every input must have the same length. We're going to use this values to padd all sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs.shape: (10000, 5)\n",
      "[  0   0  81   9 417]\n",
      "decoder_inputs.shape: (10000, 11)\n",
      "[   2  217 2365    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print(f'encoder_inputs.shape: {encoder_inputs.shape}')\n",
    "print(encoder_inputs[random_idx])\n",
    "\n",
    "decoder_inputs = pad_sequences(target_sequences_input, maxlen=max_len_target, padding='post')\n",
    "print(f'decoder_inputs.shape: {decoder_inputs.shape}')\n",
    "print(decoder_inputs[random_idx])\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Encoder we're going to use pre-trained with GloVe word embeddings. Let's load them and store them in the embedding matrix. This will help us to set the embedding layer, making use of good hyperparameters found by others. We're not going to train this layer, because the number of uncommon words i small and their effect is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors\n",
      "embedding_matrix.shape (1732, 100)\n"
     ]
    }
   ],
   "source": [
    "word2vec = {}\n",
    "for line in open(f'glove.6B.{EMBEDDING_DIM}d.txt', encoding='utf-8'):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:],dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "print(f'Found {len(word2vec)} word vectors')\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_input) + 1)\n",
    "embedding_matrix = np.zeros((num_words,EMBEDDING_DIM))\n",
    "for word,i in word2idx_input.items():\n",
    "    if i < MAX_NUM_WORDS:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "print(f'embedding_matrix.shape {embedding_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_target_one_hot.shape: (10000, 11, 5924)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_words,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_len_input\n",
    ")\n",
    "\n",
    "# in our model we will not be able to use sparce_categorical_crossentropy\n",
    "# thus we need to create OHE labels\n",
    "decoder_target_one_hot = np.zeros(\n",
    "    (\n",
    "        len(input_sequences),\n",
    "        max_len_target,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "for i,target_sequence in enumerate(decoder_targets):\n",
    "    for t,word in enumerate(target_sequence):\n",
    "        if word != 0:\n",
    "            decoder_target_one_hot[i,t,word] = 1\n",
    "print(f'decoder_target_one_hot.shape: {decoder_target_one_hot.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to create the model. The first is Encoder, which is going to take the input sequence in English, pass it through the embedding layer with pre-trained weights and then to the layer with LSTMs. We will need to make this layer to return state, as we will need it for the Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_placeholder = Input(shape=(max_len_input,))\n",
    "x = embedding_layer(encoder_input_placeholder)\n",
    "encoder = LSTM(LATENT_DIM,return_state=True)\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "# states to pass into Decoder\n",
    "encoder_states = [h,c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder's turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/40\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 5.5877 - acc: 0.3569 - val_loss: 5.3442 - val_acc: 0.3002\n",
      "Epoch 2/40\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 4.5640 - acc: 0.3809 - val_loss: 5.1896 - val_acc: 0.3182\n",
      "Epoch 3/40\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 4.1879 - acc: 0.4024 - val_loss: 5.0528 - val_acc: 0.3689\n",
      "Epoch 4/40\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 3.8832 - acc: 0.4306 - val_loss: 4.9032 - val_acc: 0.3945\n",
      "Epoch 5/40\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 3.6299 - acc: 0.4528 - val_loss: 4.8507 - val_acc: 0.4202\n",
      "Epoch 6/40\n",
      "8000/8000 [==============================] - 50s 6ms/step - loss: 3.4132 - acc: 0.4741 - val_loss: 4.7764 - val_acc: 0.4362\n",
      "Epoch 7/40\n",
      "8000/8000 [==============================] - 53s 7ms/step - loss: 3.2141 - acc: 0.4883 - val_loss: 4.7650 - val_acc: 0.4451\n",
      "Epoch 8/40\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 3.0246 - acc: 0.5058 - val_loss: 4.7462 - val_acc: 0.4546\n",
      "Epoch 9/40\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 2.8462 - acc: 0.5196 - val_loss: 4.7340 - val_acc: 0.4614\n",
      "Epoch 10/40\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 2.6751 - acc: 0.5323 - val_loss: 4.7485 - val_acc: 0.4630\n",
      "Epoch 11/40\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 2.5095 - acc: 0.5452 - val_loss: 4.7751 - val_acc: 0.4632\n",
      "Epoch 12/40\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 2.3563 - acc: 0.5603 - val_loss: 4.8039 - val_acc: 0.4679\n",
      "Epoch 13/40\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 2.2111 - acc: 0.5735 - val_loss: 4.8513 - val_acc: 0.4698\n",
      "Epoch 14/40\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 2.0731 - acc: 0.5876 - val_loss: 4.8724 - val_acc: 0.4674\n",
      "Epoch 15/40\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 1.9431 - acc: 0.6041 - val_loss: 4.9194 - val_acc: 0.4555\n",
      "Epoch 16/40\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 1.8207 - acc: 0.6183 - val_loss: 4.9429 - val_acc: 0.4551\n",
      "Epoch 17/40\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 1.7017 - acc: 0.6370 - val_loss: 4.9542 - val_acc: 0.4444\n",
      "Epoch 18/40\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 1.5896 - acc: 0.6499 - val_loss: 5.0029 - val_acc: 0.4395\n",
      "Epoch 19/40\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 1.4860 - acc: 0.6661 - val_loss: 5.0111 - val_acc: 0.4268\n",
      "Epoch 20/40\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 1.3849 - acc: 0.6788 - val_loss: 5.0212 - val_acc: 0.4328\n",
      "Epoch 21/40\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 1.2901 - acc: 0.6927 - val_loss: 5.0621 - val_acc: 0.4256\n",
      "Epoch 22/40\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 1.2023 - acc: 0.7080 - val_loss: 5.0601 - val_acc: 0.4322\n",
      "Epoch 23/40\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 1.1185 - acc: 0.7198 - val_loss: 5.0993 - val_acc: 0.4225\n",
      "Epoch 24/40\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 1.0441 - acc: 0.7327 - val_loss: 5.0883 - val_acc: 0.4303\n",
      "Epoch 25/40\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.9737 - acc: 0.7442 - val_loss: 5.1333 - val_acc: 0.4230\n",
      "Epoch 26/40\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.9100 - acc: 0.7530 - val_loss: 5.1576 - val_acc: 0.4216\n",
      "Epoch 27/40\n",
      "8000/8000 [==============================] - 58s 7ms/step - loss: 0.8533 - acc: 0.7613 - val_loss: 5.1845 - val_acc: 0.4154\n",
      "Epoch 28/40\n",
      "8000/8000 [==============================] - 57s 7ms/step - loss: 0.8010 - acc: 0.7674 - val_loss: 5.1878 - val_acc: 0.4184\n",
      "Epoch 29/40\n",
      "8000/8000 [==============================] - 53s 7ms/step - loss: 0.7555 - acc: 0.7747 - val_loss: 5.1804 - val_acc: 0.4243\n",
      "Epoch 30/40\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.7138 - acc: 0.7795 - val_loss: 5.2020 - val_acc: 0.4165\n",
      "Epoch 31/40\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.6752 - acc: 0.7826 - val_loss: 5.2514 - val_acc: 0.4162\n",
      "Epoch 32/40\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.6418 - acc: 0.7851 - val_loss: 5.2625 - val_acc: 0.4188\n",
      "Epoch 33/40\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.6132 - acc: 0.7876 - val_loss: 5.2540 - val_acc: 0.4207\n",
      "Epoch 34/40\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.5865 - acc: 0.7910 - val_loss: 5.2549 - val_acc: 0.4216\n",
      "Epoch 35/40\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.5621 - acc: 0.7913 - val_loss: 5.2803 - val_acc: 0.4194\n",
      "Epoch 36/40\n",
      "8000/8000 [==============================] - 52s 6ms/step - loss: 0.5422 - acc: 0.7947 - val_loss: 5.3069 - val_acc: 0.4179\n",
      "Epoch 37/40\n",
      "8000/8000 [==============================] - 51s 6ms/step - loss: 0.5245 - acc: 0.7945 - val_loss: 5.3110 - val_acc: 0.4210\n",
      "Epoch 38/40\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.5088 - acc: 0.7955 - val_loss: 5.3153 - val_acc: 0.4222\n",
      "Epoch 39/40\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.4929 - acc: 0.7969 - val_loss: 5.3464 - val_acc: 0.4189\n",
      "Epoch 40/40\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.4819 - acc: 0.7986 - val_loss: 5.3639 - val_acc: 0.4176\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import keras.backend as K\n",
    "    if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
    "        from keras.layers import CuDNNLSTM as LSTM\n",
    "        from keras.layers import CuDNNGRU as GRU\n",
    "except:\n",
    "    pass\n",
    "\n",
    "decoder_input_placeholder = Input(shape=(max_len_target,))\n",
    "embedding_decoder = Embedding(num_words_output,EMBEDDING_DIM)\n",
    "decoder_input_x = embedding_decoder(decoder_input_placeholder)\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_state=True, return_sequences=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = Model([encoder_input_placeholder,decoder_input_placeholder], decoder_outputs)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # both are of shape N x T x K\n",
    "    mask = K.cast(y_true > 0, dtype='float32')\n",
    "    out = mask * y_true * K.log(y_pred)\n",
    "    return -K.sum(out) / K.sum(mask)\n",
    "\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "    # both are of shape N x T x K\n",
    "    targ = K.argmax(y_true, axis=-1)\n",
    "    pred = K.argmax(y_pred, axis=-1)\n",
    "    correct = K.cast(K.equal(targ, pred), dtype='float32')\n",
    "\n",
    "    # 0 is padding, don't include those\n",
    "    mask = K.cast(K.greater(targ, 0), dtype='float32')\n",
    "    n_correct = K.sum(mask * correct)\n",
    "    n_total = K.sum(mask)\n",
    "    return n_correct / n_total\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=[acc])\n",
    "\n",
    "r = model.fit([encoder_inputs,decoder_inputs], decoder_target_one_hot, batch_size=BATCH_SIZE, epochs=EPOCHS,validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction we need to create another model with the use of previously trained layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_input_placeholder, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "decoder_states_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_input_single = Input(shape=(1,))\n",
    "decoder_input_single_x = embedding_decoder(decoder_input_single)\n",
    "decoder_outputs, h, c = decoder_lstm(decoder_input_single_x,initial_state=decoder_states_input)\n",
    "\n",
    "decoder_states = [h,c]\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_input_single] + decoder_states_input, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to map back from word index into the word itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_eng = {v:k for k,v in word2idx_input.items()}\n",
    "idx2word_rus = {v:k for k,v in word2idx_output.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "  # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "  # Populate the first character of target sequence with the start character.\n",
    "  # NOTE: tokenizer lower-cases all words\n",
    "    target_seq[0, 0] = word2idx_output['<sos>']\n",
    "\n",
    "  # if we get this we break\n",
    "    eos = word2idx_output['<eos>']\n",
    "\n",
    "  # Create the translation\n",
    "    output_sentence = []\n",
    "    for _ in range(max_len_target):\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "          [target_seq] + states_value\n",
    "        )\n",
    "    # output_tokens, h = decoder_model.predict(\n",
    "    #     [target_seq] + states_value\n",
    "    # ) # gru\n",
    "\n",
    "    # Get next word\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "    # End sentence of EOS\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            word = idx2word_rus[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "    # Update the decoder input\n",
    "    # which is just the word just generated\n",
    "        target_seq[0, 0] = idx\n",
    " \n",
    "    # Update states\n",
    "        states_value = [h, c]\n",
    "    # states_value = [h] # gru\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input: I'm with him.\n",
      "Translation: я с ним.\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input: Are you up?\n",
      "Translation: ты встал?\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input: Take a nap.\n",
      "Translation: вздремните.\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input: He gave up.\n",
      "Translation: он сдался.\n",
      "Continue? [Y/n]n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "  # Do some test translations\n",
    "    i = np.random.choice(len(input_texts))\n",
    "    input_seq = encoder_inputs[i:i+1]\n",
    "    translation = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input:', input_texts[i])\n",
    "    print('Translation:', translation)\n",
    "\n",
    "    ans = input(\"Continue? [Y/n]\")\n",
    "    if ans and ans.lower().startswith('n'):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
